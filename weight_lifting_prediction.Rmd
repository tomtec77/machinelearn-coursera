---
title: "Using accelerometer data to predict proficiency in weight lifting"
author: "Tom√°s E. Tecce"
output:
  html_document:
    toc: true
    theme: flatly
    highlights: pygments
---

# Summary

In this project I use accelerometer data collected from several users
doing barbell lifts in different ways to build a predictive model which
identifies how the exercise is being done. The original data includes 159
variables. By eliminating columns which mostly consist of empty values,
and then using principal component analysis, I obtain 25 features which
are then used to build a random forest model. The model obtained,
evaluated on a sample separated for calibration from the training set, has
an out of sample error of at least 5.53 per cent.


# Source data

Devices such as Jawbone Up, Nike FuelBand, and Fitbit allow users to
easily collect large amounts of data about their personal fitness
activities. The data can be used not only to quantify how much of a
particular activity the users do, but also to determine how well the
exercise is done.

The data set for this project comes from
http://groupware.les.inf.puc-rio.br/har.  It consists of data from
accelerometers on the belt, forearm, arm, and dumbell of 6 participants,
who were asked to perform barbell lifts correctly and incorrectly in 5
different ways. First, I downloaded the training data set and load it into
R as a data frame.

```{r}
if (!file.exists("data")) { dir.create("data") }
if (!file.exists("data/pml-training.csv")) {
  fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(fileUrl, destfile="data/pml-training.csv", method="curl")
}

dftrain <- read.table("data/pml-training.csv", sep=",",
                      stringsAsFactors=FALSE, header=TRUE,
                      na.strings=c("","NA"))
```

# Feature selection

The training data set consists of `r dim(dftrain)[1]` observations of
`r dim(dftrain)[2]` variables. I start by checking for missing data.

```{r, echo=FALSE}
na_check <- as.data.frame(sapply(dftrain, function(x) { sum(is.na(x)) }))
```

Counting the number of NA values in each column I find that the data has
either columns with no missing data, or mostly empty columns (all with the
same value of `r max(100*na_check/dim(dftrain)[1])` per cent of NAs). I 
discard the latter columns from the training set, and also columns 1
through 7 which only contain data labels and time stamps.

```{r}
features.list <- rownames(na_check)[na_check==0][-(1:7)]
dftrain <- dftrain[,features.list]
```

I'm left with 52 features and one outcome variable (`classe`). All the
features are numeric, and the outcome is of class character with five
possible values ("A", "B", "C", "D" and "E"). At this point I split the
original training data into training and calibration datasets.

```{r}
library(caret)

set.seed(59433)
trainset <- createDataPartition(y=dftrain$classe, p=0.6, list=FALSE)
dftrain1 <- dftrain[trainset,]
dfcal1   <- dftrain[-trainset,]
```

The next step is to check for correlation among these features. The
motions performed during exercising are not completely independent of each
other, and in principle it is possible for the measurements in the
different accelerometers to be correlated when performing the barbell lift
in a certain way. The correlation test is performed on the training subset
only.

```{r correlationtest}
corr_test <- cor(dftrain1[,-53])

library(corrplot)
corrplot(corr_test, method="square", order="FPC",
         tl.col="black", tl.srt=45, tl.cex=0.5)
```

The correlation plot above shows that there are indeed several features
with a strong degree of correlation with others. For example, `roll_belt`
is strongly correlated with `yaw_belt`, `total_accel_belt` and
`accel_belt_y` (correlation larger than 0.75):

```{r}
corr_test[1,][corr_test[1,]>0.75]
```

When this happens a useful strategy is to preprocess the data with
principal component analysis (PCA) to reduce the number of predictors,
generating weighted combinations of features which capture the most
information possible. Once the principal components are determined from
the training subset, they are applied to the calibration set too.

```{r, message=FALSE}
trainpca <- preProcess(dftrain1[,-53], method="pca")
dftrainpca <- predict(trainpca, dftrain1[,-53])
dfcalpca <- predict(trainpca, dfcal1[,-53])
```

I have selected the default threshold of 95 per cent for the cumulative
percent of variance to be retained by the PCA. That choice yields 25 PCA
components, a significant reduction in the number of features.


# Model construction

To construct my predictive model I have chosen the random forest method.

In order to avoid memory allocation errors in my machine, I had to
restrict the size of my training sets to less than 7,000 observations. I
also had to limit myself to building relatively small forests (250 trees)
because larger forests take too long to compute.

```{r modelbuild, message=FALSE, cache=TRUE}
dftrainpca$classe <- dftrain1$classe
dfcalpca$classe <- dfcal1$classe

smallTrain <- createDataPartition(y=dftrainpca$classe, p=0.55, list=FALSE)
dftrain2 <- dftrainpca[smallTrain,]
my.model <- train(classe ~ ., data=dftrain2, method="rf", ntree=250,
                  trControl=trainControl(method="cv", number=4), prox=TRUE)
my.model$finalModel
```

By training the model with cross validation, I obtain an estimate for the
out of box (OOB) error, 5.87% in this case. Even with the restrictions I
had to impose due to my machine's performance, I obtain a very accurate
model (OOB at least).

# Model validation

I now apply the trained model on the calibration set, using the `predict`
function. For consistency, I only use a partition of the calibration data
(the same proportion I used to create the smaller training set), although
this is probably not necessary. The output of the `confusionMatrix`
function shows how well the model classified the calibration data,
together with several accuracy measures.

```{r}
smallCal <- createDataPartition(y=dfcalpca$classe, p=0.55, list=FALSE)
dfcal2 <- dfcalpca[smallCal,]
confusionMatrix(dfcal2$class, predict(my.model, newdata=dfcal2))
```

The out of sample (OOS) error for this model is estimated from this as 1
minus the accuracy, so in this case I get that the OOS error for the model
built is at least 5.53 per cent.

# Predicting on the test set

The final step is to download the test data and use the model on it to
make predictions. 

```{r}
if (!file.exists("data/pml-testing.csv")) {
  fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  download.file(fileUrl, destfile="data/pml-testing.csv", method="curl")
}

dftest <- read.table("data/pml-testing.csv", sep=",",
                     stringsAsFactors=FALSE, header=TRUE,
                      na.strings=c("","NA"))
dim(dftest)

dftest <- dftest[,c(features.list[-53],"problem_id")]
dftestpca <- predict(trainpca, dftest[,-53])
dftestpca$classe <- dftest$classe

final.prediction <- predict(my.model, newdata=dftestpca)
```

To complete the assignment I have to create individual text files for each
prediction.

```{r}
pml_write_files <- function(x, outpath="./") {
  n <- length(x)
  for (i in 1:n) {
    filename <- paste0(outpath, "problem_id_", i, ".txt")
    write.table(x[i], file=filename, quote=FALSE, row.names=FALSE,
                col.names=FALSE)
  }
}

if (!file.exists("output")) { dir.create("output") }
pml_write_files(as.character(final.prediction), outpath="output/")
```

# Conclusion

In this project I created a model to predict the type of physical exercise
based on accelerometer data. The final score obtained on the test data was
17 correct guesses out of 20 cases, 85 per cent accuracy. Here we see that
the actual model error is larger than the estimated OOS error, but still
much better than random guesses.

Random forest models are very accurate, but take a long time to train. In
this case, because of the limited performance of my machine I had to
restrict myself to train in a smaller sample and with a relatively small
forest of only 250 trees. Nevertheless, the final results were very
accurate; with more time and/or computing power, I am confident that the
model accuracy can be increased.
