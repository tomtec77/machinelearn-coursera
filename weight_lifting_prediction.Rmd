---
title: "Predicting proficiency in weight lifting"
author: "Tom√°s E. Tecce"
output:
  html_document:
    toc: true
    theme: flatly
    highlights: pygments
---

# Summary

# Source data

The data set for this project comes from
http://groupware.les.inf.puc-rio.br/har.  It consists of data from
accelerometers on the belt, forearm, arm, and dumbell of 6 participants,
who were asked to perform barbell lifts correctly and incorrectly in 5
different ways. First, I downloaded the training data set and load it into
R as a data frame.

```{r}
if (!file.exists("data")) { dir.create("data") }
if (!file.exists("data/pml-training.csv")) {
  fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(fileUrl, destfile="data/pml-training.csv", method="curl")
}

dftrain <- read.table("data/pml-training.csv", sep=",",
                      stringsAsFactors=FALSE, header=TRUE,
                      na.strings=c("","NA"))
```

# Feature selection

The training data set consists of `r dim(dftrain)[1]` observations of
`r dim(dftrain)[2]` variables. I start by checking for missing data.

```{r, echo=FALSE}
na_check <- as.data.frame(sapply(dftrain, function(x) { sum(is.na(x)) }))
```

Counting the number of NA values in each column I find that the data has
either columns with no missing data, or mostly empty columns (all with the
same value of `r max(100*na_check/dim(dftrain)[1])` per cent of NAs). I 
discard the latter columns from the training set, and also columns 1
through 7 which only contain data labels and time stamps.

```{r}
features.list <- rownames(na_check)[na_check==0][-(1:7)]
dftrain <- dftrain[,features.list]
```

I'm left with 52 features and one outcome variable (`classe`). All the
features are numeric, and the outcome is of class character with five
possible values ("A", "B", "C", "D" and "E"). At this point I split the
original training data into training and calibration datasets.

```{r}
library(caret)

set.seed(59433)
trainset <- createDataPartition(y=dftrain$classe, p=0.6, list=FALSE)
dftrain1 <- dftrain[trainset,]
dfcal1   <- dftrain[-trainset,]
```

The next step is to check for correlation among these features. The
motions performed during exercising are not completely independent of each
other, and in principle it is possible for the measurements in the
different accelerometers to be correlated when performing the barbell lift
in a certain way. The correlation test is performed on the training subset
only.

```{r}
corr_test <- cor(dftrain1[,-53])

library(corrplot)
corrplot(corr_test, method="square", order="FPC",
         tl.col="black", tl.srt=45, tl.cex=0.5)
```

The correlation plot above shows that there are indeed several features
with a strong degree of correlation with others. For example, `roll_belt`
is strongly correlated with `yaw_belt`, `total_accel_belt` and
`accel_belt_y` (correlation larger than 0.75):

```{r}
corr_test[1,][corr_test[1,]>0.75]
```

When this happens a useful strategy is to preprocess the data with
principal component analysis (PCA) to reduce the number of predictors,
generating weighted combinations of features which capture the most
information possible. Once the principal components are determined from
the training subset, they are applied to the calibration set too.

```{r, message=FALSE}
trainpca <- preProcess(dftrain1[,-53], method="pca")
dftrainpca <- predict(trainpca, dftrain1[,-53])
dfcalpca <- predict(trainpca, dfcal1[,-53])
```

I have selected the default threshold of 95 per cent for the cumulative
percent of variance to be retained by the PCA. That choice yields 25 PCA
components, a significant reduction in the number of features.


# Model construction

To construct my predictive model I have chosen the random forest method.

In order to avoid memory allocation errors in my machine, I had to
restrict the size of my training sets to less than 7,000 observations. I
also had to limit myself to building relatively small forests (250 trees)
because larger forests take too long to compute.

```{r modelbuild, message=FALSE, cache=TRUE}
dftrainpca$classe <- dftrain1$classe
dfcalpca$classe <- dfcal1$classe

smallTrain <- createDataPartition(y=dftrainpca$classe, p=0.55, list=FALSE)
dftrain2 <- dftrainpca[smallTrain,]
my.model <- train(classe ~ ., data=dftrain2, method="rf", ntree=250,
                  trControl=trainControl(method="cv", number=4), prox=TRUE)
my.model$finalModel
```

By training the model with cross validation, I obtain an estimate for the
out of box (OOB) error, 5.87% in this case. Even with the restrictions I
had to impose due to my machine's performance, I obtain a very accurate
model (OOB at least).

# Model validation

I now apply the trained model on the calibration set, using the `predict`
function. For consistency, I only use a partition of the calibration data
(the same proportion I used to create the smaller training set), although
this is probably not necessary. The output of the `confusionMatrix`
function shows how well the model classified the calibration data,
together with several accuracy measures.

```{r}
smallCal <- createDataPartition(y=dfcalpca$classe, p=0.55, list=FALSE)
dfcal2 <- dfcalpca[smallCal,]
confusionMatrix(dfcal2$class, predict(my.model, newdata=dfcal2))
```

The out of sample (OOS) error for this model is estimated from this as 1
minus the accuracy, so in this case I get that the OOS error for the model
built is at least 5.53 per cent.